{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "987f6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3f0d17e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.rallit.com/?jobSkillKeywords=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeyword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&pageNumber=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     19\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m---> 20\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# 페이지가 완전히 로드될 때까지 잠시 대기\u001b[39;00m\n\u001b[0;32m     22\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(driver\u001b[38;5;241m.\u001b[39mpage_source, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m검색결과가 없어요\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mtext:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "keyword = \"AWS\"\n",
    "page = 1\n",
    "main_url = \"https://www.rallit.com\"\n",
    "rallit_res = pd.DataFrame()\n",
    "\n",
    "# 웹 드라이버 초기화\n",
    "driver = webdriver.Chrome()  # 혹은 다른 브라우저에 맞게 변경\n",
    "\n",
    "while True:\n",
    "    url = f'https://www.rallit.com/?jobSkillKeywords={keyword}&pageNumber={page}'\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # 페이지가 완전히 로드될 때까지 잠시 대기\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    if \"검색결과가 없어요\" in soup.text:\n",
    "        break\n",
    "\n",
    "    # 각 공고를 클릭하여 정보 수집\n",
    "    job_elements = driver.find_elements(By.XPATH, '//*[@id=\"__next\"]/main/div/section[3]/ul/li/article/a/figure/img')\n",
    "    for i in range(len(job_elements)):\n",
    "        job_elements = driver.find_elements(By.XPATH, '//*[@id=\"__next\"]/main/div/section[3]/ul/li/article/a/figure/img')\n",
    "        job_element = job_elements[i]\n",
    "        job_element.click()\n",
    "\n",
    "        # 공고 페이지가 완전히 로드될 때까지 대기\n",
    "        WebDriverWait(driver, 10).until(EC.url_changes(driver.current_url))\n",
    "\n",
    "        sub_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        corp_name = sub_soup.select_one('h2').text.strip() if sub_soup.select_one('h2') else \"\" # 기업명\n",
    "        address = sub_soup.select_one('p.css-1hdnp7d').text.strip() if sub_soup.select_one('p.css-1hdnp7d') else \"\"  # 주소\n",
    "        career = sub_soup.select_one('dd.css-1pvdrt3').text.strip() if sub_soup.select_one('dd.css-1pvdrt3') else \"\"  # 직급\n",
    "        job_description = sub_soup.select('h1.css-17ueevk')[0].text.strip()  # 직무\n",
    "        job_skill = [keyword.text.strip()[1:] if keyword.text.strip().startswith('#') else keyword.text.strip() for keyword in sub_soup.select('div.css-175wty3')]\n",
    "        job_preferential = sub_soup.select('p.css-19hzmb1')[4].text.strip() if len(sub_soup.select('p.css-19hzmb1')) > 4 else \"\"  # 우대사항\n",
    "\n",
    "        employment_url = driver.current_url #해당 페이지 URL\n",
    "        job_requirements = sub_soup.select('p.css-19hzmb1')[3].text.strip() if len(sub_soup.select('p.css-19hzmb1')) > 3 else \"\"  # 자격요건\n",
    "\n",
    "        temp_dict = {\n",
    "            '기업명': corp_name,\n",
    "            '주소': address,\n",
    "            '직무': job_description,\n",
    "            '직급': career,\n",
    "            '이용하는 기술스택': job_skill,\n",
    "            '우대사항': job_preferential,\n",
    "            '해당 페이지 URL': employment_url,\n",
    "            '자격요건': job_requirements\n",
    "        }\n",
    "\n",
    "        temp_df = pd.DataFrame([temp_dict])\n",
    "        rallit_res = pd.concat([rallit_res, temp_df], ignore_index=True)\n",
    "\n",
    "        # 뒤로 가기\n",
    "        driver.back()\n",
    "        time.sleep(3)  # 페이지가 완전히 로드될 때까지 잠시 대기\n",
    "\n",
    "    page += 1\n",
    "\n",
    "# 크롤링이 끝나면 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "# 결과 확인\n",
    "print(rallit_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0feef9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 CSV 파일로 저장\n",
    "rallit_res.to_csv('rallit_0.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cccd9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Downloads\\crawling_project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
